{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijayamurugan27/GenerativeAI/blob/main/Advanced_Agents_use.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q langchain_groq langchain_community gradio langgraph nasapy polygon arxiv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFitQqx5g-dr",
        "outputId": "42480e61-1028-451d-c08b-ffeaeef714c1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.1/125.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.9/111.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "llm_groq = ChatGroq(model_name=\"llama3-groq-70b-8192-tool-use-preview\", api_key=userdata.get(\"GROQ_API_KEY\"))\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_SEARCH\")\n",
        "os.environ[\"POLYGON_API_KEY\"] = userdata.get(\"POLYGON_API_KEY\")"
      ],
      "metadata": {
        "id": "zdmDTQKTga8j"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import json\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.prebuilt import create_react_agent"
      ],
      "metadata": {
        "id": "Zl6LuJ_dgebN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool # For custom tools\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_community.tools.arxiv import ArxivQueryRun\n",
        "from langchain_community.utilities.polygon import PolygonAPIWrapper\n",
        "from langchain_community.tools.polygon import PolygonFinancials, PolygonAggregates, PolygonTickerNews, PolygonLastQuote\n",
        "from langchain_community.tools.youtube.search import YouTubeSearchTool\n",
        "from nasapy import Nasa\n",
        "import polygon"
      ],
      "metadata": {
        "id": "ORPMc5CjgpmE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QlVOyijTgWAq"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "We will first setup the tools needed to run by the agent. We will use search engine API call\n",
        "and one custom function that will at this point do nothing.\n",
        "\n",
        "You can find standard list of tools here: https://python.langchain.com/v0.2/docs/integrations/tools/\n",
        "A lot of them are noise, but some of them are useful.\n",
        "\"\"\"\n",
        "search = TavilySearchResults(max_results=2)\n",
        "paper = ArxivQueryRun()\n",
        "yt = YouTubeSearchTool()\n",
        "stocks_client = polygon.StocksClient(os.getenv(\"POLYGON_API_KEY\"))\n",
        "stock_financials = PolygonFinancials(api_wrapper = PolygonAPIWrapper())\n",
        "stock_ticker_news = PolygonTickerNews(api_wrapper = PolygonAPIWrapper())\n",
        "\n",
        "@tool\n",
        "def get_hotel(location: str, at_time: datetime)->str:\n",
        "    '''Returns hotel options for reservation'''\n",
        "    return '''There is a beautiful Motel 6 available for that date'''\n",
        "\n",
        "@tool\n",
        "def get_nasa_pic_of_day()->str:\n",
        "    \"\"\"Get NASA's Astronomy Picture of the Day today.\"\"\"\n",
        "    date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    nasa = Nasa()\n",
        "    apod = nasa.picture_of_the_day(date, hd=True)\n",
        "    return apod['url']\n",
        "\n",
        "@tool\n",
        "def get_stock_price(ticker: str)->str:\n",
        "    '''Returns the latest stock price'''\n",
        "    return stocks_client.get_previous_close(ticker)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [search, paper, stock_financials, yt, stock_ticker_news, get_hotel, get_nasa_pic_of_day, get_stock_price]\n",
        "prompt = \"\"\"We will be using tools to solve a variety of user problems. This will be displayed on Gradio.\n",
        "              The output has to be in markdown format to display properly. For weather and search you will use Tavily\"\"\"\n",
        "memory = MemorySaver()\n",
        "agent_executor = create_react_agent(llm_groq, tools, checkpointer=memory, state_modifier=prompt)\n",
        "config = {\"configurable\": {\"thread_id\": \"id111\"}} # Hardcoded -- this is actually to switch different threads of conversations"
      ],
      "metadata": {
        "id": "URnJeDP3hpN4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the function to handle chat messages from the user\n",
        "def chat(message, history):\n",
        "    tokens_used = 0\n",
        "    tool_name = None\n",
        "    query = None\n",
        "    for chunk in agent_executor.stream(\n",
        "        {\"messages\": [HumanMessage(content=message)]}, config):\n",
        "\n",
        "        #print(chunk)\n",
        "        agent_data = chunk.get(\"agent\", None)\n",
        "\n",
        "        if agent_data is not None:\n",
        "            messages = agent_data.get(\"messages\", None)\n",
        "\n",
        "            if messages is not None and len(messages) > 0:\n",
        "                message = messages[0]  # Corrected to refer to the first message\n",
        "                tool_calls = message.additional_kwargs.get(\"tool_calls\", None)\n",
        "\n",
        "                if tool_calls is not None and len(tool_calls) > 0:\n",
        "                    tool_name = tool_calls[0].get(\"function\", {}).get(\"name\", None)\n",
        "                    arguments_str = tool_calls[0].get(\"function\", {}).get(\"arguments\", '{}')\n",
        "                    #print(tool_calls)\n",
        "                    arguments = json.loads(arguments_str)  # Parse the JSON string into a dictionary\n",
        "                    query = arguments.get(\"query\", None)  # Now safely access the query\n",
        "\n",
        "                tokens_used = message.response_metadata['token_usage']['total_tokens']\n",
        "                if tool_name is not None and query is not None:\n",
        "                    yield message.content + \"\\nTokens used so far: \" + str(tokens_used)+\"\\nTool used: \"+tool_name+\"\\nQuery: \"+query\n",
        "\n",
        "                elif tool_name is not None:\n",
        "                    yield message.content + \"\\nTokens used so far: \" + str(tokens_used)+\"\\nTool used: \"+tool_name\n",
        "\n",
        "                else:\n",
        "                    yield message.content"
      ],
      "metadata": {
        "id": "GTzw22p3iGg8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat,\n",
        "    title=\"Advanced chat agent\",\n",
        ")\n",
        "demo.launch()\n",
        "\n",
        "# can you do hotel booking?\n",
        "# Get nasa's picture of the day\n",
        "# Nutrition info or home made roti\n",
        "# how many Infosys stocks can i buy for $1000?\n",
        "# financials for AAPL\n",
        "# compare that against NVDA\n",
        "# which stock is better in comparison\n",
        "# get me latest news for MSFT ticker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        },
        "id": "62SH2Cg_iJ8Z",
        "outputId": "6752ea6e-4b0b-402a-aebe-39743a91d4de"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:231: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5ea88aea8be32bd8f1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5ea88aea8be32bd8f1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}