{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijayamurugan27/GenerativeAI/blob/main/Agents_with_Langraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modern AI Pro: Agents with Langraph\n",
        "This script demonstrates a basic chat agent leveraging the integration of LangChain's LLM capabilities with a search tool.\n",
        "\n",
        "The agent is designed to handle conversational queries, such as asking about the weather, by utilizing both a memory component for maintaining conversation context and a search tool for retrieving relevant information.\n",
        "\n",
        "Key Components:\n",
        "- **LangchainMemory**: Initializes and manages the underlying language model, tailored for the Azure OpenAI deployment. The memory buffer allows the agent to retain context across interactions, improving the quality of responses over time.\n",
        "- **TavilySearchResults**: A search tool that fetches and returns search results, limited to a configurable maximum number of results. In this demo, it is set to return up to 2 results per query.\n",
        "- **MemorySaver**: A checkpoint component that saves and restores conversation state, ensuring continuity and consistency across different conversation threads.\n",
        "- **create_react_agent**: A utility that combines the LLM, search tools, and memory checkpointing into a coherent agent that can process user inputs and return intelligent, context-aware responses.\n",
        "- **Gradio Interface**: Provides a simple chat interface for interacting with the agent, where users can input messages and receive responses in real-time.\n",
        "\n",
        "Usage:\n",
        "- The `chat` function handles streaming responses from the agent. It processes the conversation in chunks to allow for partial responses, yielding the first message content from the agent.\n",
        "- The `config` object contains a hardcoded thread ID, which is used to manage and switch between different conversation threads.\n",
        "- The script ends by launching a Gradio interface where users can interact with the agent under the title \"Mitra Robot Buffer Memory Chat\".\n"
      ],
      "metadata": {
        "id": "ecXmrS1Be02S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q langchain_groq langchain_community gradio langgraph\n",
        "import os\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "llm_groq = ChatGroq(model_name=\"llama3-groq-70b-8192-tool-use-preview\", api_key=userdata.get(\"GROQ_API_KEY\"))\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_SEARCH\")"
      ],
      "metadata": {
        "id": "1Xi_6-hGesIb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.prebuilt import create_react_agent"
      ],
      "metadata": {
        "id": "Tz63hXIyfXFb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search = TavilySearchResults(max_results=2)\n",
        "tools = [search]\n",
        "\n",
        "# Create the agent\n",
        "memory = MemorySaver()\n",
        "agent_executor = create_react_agent(llm_groq, tools, checkpointer=memory)\n",
        "config = {\"configurable\": {\"thread_id\": \"id111\"}} # Hardcoded -- this is actually to switch different threads of conversations"
      ],
      "metadata": {
        "id": "Z0wbCEP6fkXo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        },
        "id": "HdxYJI7Kef0F",
        "outputId": "4eded965-0add-434b-acd3-916ddfca3a02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:231: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8cade38f1a3ab0cfb7.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8cade38f1a3ab0cfb7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import gradio as gr\n",
        "def chat(message, history):\n",
        "    for chunk in agent_executor.stream(\n",
        "        {\"messages\": [HumanMessage(content=message)]}, config):\n",
        "        agent_data = chunk.get(\"agent\", None)\n",
        "\n",
        "        if agent_data is not None:\n",
        "            messages = agent_data.get(\"messages\", None)\n",
        "\n",
        "        if messages is not None and len(messages) > 0:\n",
        "            yield messages[0].content\n",
        "\n",
        "# Setup Gradio\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat,\n",
        "    title=\"Mitra Robot Search Engine powered chat\",\n",
        ")\n",
        "demo.launch()\n",
        "\n",
        "# You can ask something like:\n",
        "# What is today's weather in Mumbai?\n",
        "# Give me the local news there.\n",
        "# Can you give me today's news in India and say in Navjot Sidhu's tone?"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p9OYVZfA6JDA"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}